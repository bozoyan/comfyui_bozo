import os
import json
import torch
import safetensors.torch
import folder_paths

class B_QwenLoraConverterNode:

    @classmethod
    def INPUT_TYPES(cls):

        return {
            "required": {
                "lora_file": (folder_paths.get_filename_list("loras"),),
            },
        }
    
    RETURN_TYPES = ()
    FUNCTION = "convert_lora"
    OUTPUT_NODE = True
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/åŠŸèƒ½"

    def convert_lora(self, lora_file):
        # è·å–LoRAæ–‡ä»¶å®Œæ•´è·¯å¾„
        lora_dir = folder_paths.get_folder_paths("loras")[0]
        lora_path = os.path.join(lora_dir, lora_file)
        
        print(f"æ­£åœ¨å¤„ç†LoRAæ–‡ä»¶: {lora_path}")
        
        # è¯»å–LoRAæ–‡ä»¶
        try:
            if lora_path.endswith('.safetensors'):
                lora_data = safetensors.torch.load_file(lora_path, device="cpu")
            else:
                lora_data = torch.load(lora_path, map_location="cpu")
        except Exception as e:
            print(f"è¯»å–LoRAæ–‡ä»¶å¤±è´¥: {e}")
            return ()
        
        # # æ‰“å°æ‰€æœ‰keyå€¼
        # print("LoRAæ–‡ä»¶åŒ…å«çš„keyå€¼:")
        # for key in lora_data.keys():
        #     print(f"  {key}")
        
        # è½¬æ¢keyå€¼æ ¼å¼
        converted_dict = self._convert_keys(lora_data)
        converted_dict = self._fix_prefix(converted_dict)
        
        # ä¿å­˜è½¬æ¢åçš„æ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(lora_file))[0]
        output_filename = f"{base_name}_converted.safetensors"
        output_path = os.path.join(os.path.dirname(lora_path), output_filename)
        
        try:
            safetensors.torch.save_file(converted_dict, output_path)
            print(f"è½¬æ¢åçš„LoRAæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            print(f"ä¿å­˜è½¬æ¢åçš„LoRAæ–‡ä»¶å¤±è´¥: {e}")
            return ()
        
        return ()
    
    
    def _convert_keys(self, lora_data):
        converted_dict = {}
        for key, value in lora_data.items():
            if 'lora_' not in key:
                converted_dict[key] = value
                continue

            fixed_key = key.replace(".default.weight", "")
            
            if fixed_key.endswith(".lora_A"):
                fixed_key = fixed_key.replace(".lora_A", ".lora.down.weight")
            elif fixed_key.endswith(".lora_B"):
                fixed_key = fixed_key.replace(".lora_B", ".lora.up.weight")
            else:
                continue
                
            full_key = "diffusion_model." + fixed_key
            converted_dict[full_key] = value
        
        return converted_dict
    

    def _fix_prefix(self, lora_data):
        converted_dict = {}
        for key, value in lora_data.items():
            if key.startswith("transformer_blocks."):
                fixed_key = "diffusion_model." + key
                converted_dict[fixed_key] = value
            elif key.startswith("transformer.transformer_blocks"):
               fixed_key = key.replace("transformer.transformer_blocks", "diffusion_model.transformer_blocks", 1)
               converted_dict[fixed_key] = value
            else:
                converted_dict[key] = value
                continue
       
        return converted_dict


# æ–°å¢çš„LoRAæ¨¡å‹è½¬æ¢ç±»ï¼Œå‚è€ƒAPI.mdä¸­çš„patch_comfyui_nunchaku_lora.pyåŠŸèƒ½
class B_NunchakuLoraConverterNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "lora_file": (folder_paths.get_filename_list("loras"),),
            },
        }
    
    RETURN_TYPES = ()
    FUNCTION = "convert_lora"
    OUTPUT_NODE = True
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/åŠŸèƒ½"

    def convert_lora(self, lora_file):
        # è·å–LoRAæ–‡ä»¶å®Œæ•´è·¯å¾„
        lora_dir = folder_paths.get_folder_paths("loras")[0]
        input_path = os.path.join(lora_dir, lora_file)
        
        print(f"æ­£åœ¨å¤„ç†LoRAæ–‡ä»¶: {input_path}")
        
        # è¯»å–LoRAæ–‡ä»¶
        try:
            if input_path.endswith('.safetensors'):
                state_dict = safetensors.torch.load_file(input_path, device="cpu")
            else:
                state_dict = torch.load(input_path, map_location="cpu")
        except Exception as e:
            print(f"è¯»å–LoRAæ–‡ä»¶å¤±è´¥: {e}")
            return ()
        
        print(f"âœ… Loaded {len(state_dict)} tensors from: {input_path}")

        # å¤„ç†final_layerç›¸å…³çš„æƒé‡
        state_dict = self.patch_final_layer_adaLN(state_dict, prefix="lora_unet_final_layer")
        state_dict = self.patch_final_layer_adaLN(state_dict, prefix="final_layer")
        state_dict = self.patch_final_layer_adaLN(state_dict, prefix="base_model.model.final_layer")

        # ä¿å­˜è½¬æ¢åçš„æ–‡ä»¶
        base_name = os.path.splitext(os.path.basename(lora_file))[0]
        output_filename = f"{base_name}_nunchaku.safetensors"
        output_path = os.path.join(os.path.dirname(input_path), output_filename)
        
        try:
            safetensors.torch.save_file(state_dict, output_path)
            print(f"âœ… Patched file saved to: {output_path}")
            print(f"   Total tensors now: {len(state_dict)}")
        except Exception as e:
            print(f"ä¿å­˜è½¬æ¢åçš„LoRAæ–‡ä»¶å¤±è´¥: {e}")
            return ()
        
        return ()

    def patch_final_layer_adaLN(self, state_dict, prefix="lora_unet_final_layer", verbose=True):
        """
        Add dummy adaLN weights if missing, using final_layer_linear shapes as reference.
        Args:
            state_dict (dict): keys -> tensors
            prefix (str): base name for final_layer keys
            verbose (bool): print debug info
        Returns:
            dict: patched state_dict
        """
        final_layer_linear_down = None
        final_layer_linear_up = None

        adaLN_down_key = f"{prefix}_adaLN_modulation_1.lora_down.weight"
        adaLN_up_key = f"{prefix}_adaLN_modulation_1.lora_up.weight"
        linear_down_key = f"{prefix}_linear.lora_down.weight"
        linear_up_key = f"{prefix}_linear.lora_up.weight"

        if verbose:
            print(f"\nğŸ” Checking for final_layer keys with prefix: '{prefix}'")
            print(f"   Linear down: {linear_down_key}")
            print(f"   Linear up:   {linear_up_key}")

        if linear_down_key in state_dict:
            final_layer_linear_down = state_dict[linear_down_key]
        if linear_up_key in state_dict:
            final_layer_linear_up = state_dict[linear_up_key]

        has_adaLN = adaLN_down_key in state_dict and adaLN_up_key in state_dict
        has_linear = final_layer_linear_down is not None and final_layer_linear_up is not None

        if verbose:
            print(f"   âœ… Has final_layer.linear: {has_linear}")
            print(f"   âœ… Has final_layer.adaLN_modulation_1: {has_adaLN}")

        if has_linear and not has_adaLN:
            # åˆ›å»ºä¸linearæƒé‡ç›¸åŒå½¢çŠ¶çš„adaLNæƒé‡
            # ä½¿ç”¨å°çš„éé›¶å€¼è€Œä¸æ˜¯å®Œå…¨ä¸ºé›¶ï¼Œä»¥ç¡®ä¿æƒé‡è¢«æ­£ç¡®è¯†åˆ«
            dummy_down = torch.randn_like(final_layer_linear_down) * 0.001
            dummy_up = torch.randn_like(final_layer_linear_up) * 0.001
            
            # ç¡®ä¿æƒé‡è¢«æ­£ç¡®æ·»åŠ 
            state_dict[adaLN_down_key] = dummy_down
            state_dict[adaLN_up_key] = dummy_up

            if verbose:
                print(f"âœ… Added adaLN weights with small random values:")
                print(f"   {adaLN_down_key} (shape: {dummy_down.shape})")
                print(f"   {adaLN_up_key} (shape: {dummy_up.shape})")
                print(f"   Note: Using small random values instead of zeros for better compatibility")
        else:
            if verbose:
                print("âœ… No patch needed â€” adaLN weights already present or no final_layer.linear found.")

        return state_dict


# # æ³¨å†ŒèŠ‚ç‚¹
# NODE_CLASS_MAPPINGS = {
#     "B_QwenLoraConverterNode": B_QwenLoraConverterNode
# }

# NODE_DISPLAY_NAME_MAPPINGS = {
#     "B_QwenLoraConverterNode": "Qwen-Image Lora Converter"
# }