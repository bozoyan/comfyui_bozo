import os
import json
import base64
import random
from zhipuai import ZhipuAI
from PIL import Image
import numpy as np
import io

# --- å…¨å±€å¸¸é‡å’Œé…ç½® ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_FILE_NAME = 'key/glm.json'

# æç¤ºè¯æ–‡ä»¶åç§°ï¼ˆç°åœ¨æ˜¯TXTæ–‡ä»¶ï¼Œä½†å†…éƒ¨æœ‰ç‰¹å®šæ ¼å¼ï¼‰
TEXT_PROMPTS_FILE_NAME = 'text_prompts.txt'
IMAGE_PROMPTS_FILE_NAME = 'image_prompts.txt'

# æ”¯æŒçš„è¯­è¨€ä»£ç åˆ—è¡¨ï¼Œç”¨äºç¿»è¯‘èŠ‚ç‚¹
# æ™ºè°±AIçš„ç¿»è¯‘èƒ½åŠ›é€šå¸¸æ˜¯é€šç”¨è¯­è¨€å¯¹ï¼Œè¿™é‡Œåˆ—å‡ºä¸€äº›å¸¸è§è¯­è¨€ä½œä¸ºç¤ºä¾‹
# å®é™…æ”¯æŒçš„è¯­è¨€å¯èƒ½éœ€è¦æŸ¥é˜…æ™ºè°±AIå®˜æ–¹æ–‡æ¡£
SUPPORTED_TRANSLATION_LANGS = [
    'zh', 'en',
    # æ›´å¤šè¯­è¨€å¯ä»¥æ ¹æ®æ™ºè°±AIå®é™…æ”¯æŒæƒ…å†µæ·»åŠ 
]

# --- è¾…åŠ©å‡½æ•° ---

def _log_info(message):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°"""
    print(f"[GLM_Nodes] ä¿¡æ¯ï¼š{message}")

def _log_warning(message):
    """ç»Ÿä¸€çš„è­¦å‘Šè¾“å‡ºå‡½æ•°"""
    print(f"[GLM_Nodes] è­¦å‘Šï¼š{message}")

def _log_error(message):
    """ç»Ÿä¸€çš„é”™è¯¯è¾“å‡ºå‡½æ•°"""
    print(f"[GLM_Nodes] é”™è¯¯ï¼š{message}")

def get_zhipuai_api_key():
    """
    å°è¯•ä»ç¯å¢ƒå˜é‡ ZHIPUAI_API_KEY è·å–æ™ºè°±AI API Keyã€‚
    å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä»keyç›®å½•ä¸‹çš„ glm.json æ–‡ä»¶ä¸­è¯»å–ã€‚
    è¿”å› API Key å­—ç¬¦ä¸²ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    env_api_key = os.getenv("ZHIPUAI_API_KEY")
    if env_api_key:
        _log_info("ä½¿ç”¨ç¯å¢ƒå˜é‡ API Keyã€‚")
        return env_api_key

    config_path = os.path.join(CURRENT_DIR, CONFIG_FILE_NAME)
    try:
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            api_key = config.get("ZHIPUAI_API_KEY")
            if api_key:
                _log_info(f"ä» {CONFIG_FILE_NAME} è¯»å– API Keyã€‚")
                return api_key
            else:
                _log_warning(f"åœ¨ {CONFIG_FILE_NAME} ä¸­æœªæ‰¾åˆ° ZHIPUAI_API_KEYã€‚")
                return ""
        else:
            _log_warning(f"æœªæ‰¾åˆ° API Key é…ç½®æ–‡ä»¶ {CONFIG_FILE_NAME}ã€‚")
            return ""
    except json.JSONDecodeError:
        _log_error(f"é…ç½®æ–‡ä»¶ {CONFIG_FILE_NAME} æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return ""
    except Exception as e:
        _log_error(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return ""

def load_prompts_from_txt(file_path, default_built_in_prompts):
    """
    ä»ç‰¹å®šæ ¼å¼çš„TXTæ–‡ä»¶åŠ è½½å¤šä¸ªæç¤ºè¯ã€‚
    æ ¼å¼è¦æ±‚ï¼šæ¯ä¸ªæç¤ºè¯ä»¥ `[æç¤ºè¯åç§°]` å¼€å¤´ï¼Œå†…å®¹åœ¨å…¶åï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª `[` å¼€å¤´æˆ–æ–‡ä»¶ç»“æŸã€‚
    ç©ºè¡Œå’Œè¡Œé¦–è¡Œå°¾çš„ç©ºæ ¼ä¼šè¢«å»é™¤ã€‚
    """
    prompts = {}
    current_prompt_name = None
    current_prompt_content = []

    if not os.path.exists(file_path):
        _log_warning(f"æç¤ºè¯æ–‡ä»¶ '{os.path.basename(file_path)}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤æç¤ºè¯ã€‚")
        return default_built_in_prompts

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip() # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
                if not line: # è·³è¿‡ç©ºè¡Œ
                    continue

                if line.startswith('[') and line.endswith(']'):
                    # æ–°çš„æç¤ºè¯åç§°
                    if current_prompt_name and current_prompt_content:
                        prompts[current_prompt_name] = "\n".join(current_prompt_content).strip()
                
                    current_prompt_name = line[1:-1].strip() # æå–åç§°
                    current_prompt_content = [] # é‡ç½®å†…å®¹
                elif current_prompt_name is not None:
                    # æ·»åŠ å†…å®¹åˆ°å½“å‰æç¤ºè¯
                    current_prompt_content.append(line)
                # else: å¿½ç•¥æ–‡ä»¶å¼€å¤´åœ¨ç¬¬ä¸€ä¸ª [ ] ä¹‹å‰çš„è¡Œ

            # å¤„ç†æ–‡ä»¶æœ«å°¾çš„æœ€åä¸€ä¸ªæç¤ºè¯
            if current_prompt_name and current_prompt_content:
                prompts[current_prompt_name] = "\n".join(current_prompt_content).strip()

        if not prompts:
            _log_warning(f"æç¤ºè¯æ–‡ä»¶ '{os.path.basename(file_path)}' å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤æç¤ºè¯ã€‚")
            return default_built_in_prompts

        # _log_info(f"ä» '{os.path.basename(file_path)}' åŠ è½½æç¤ºè¯æˆåŠŸã€‚")
        return prompts

    except Exception as e:
        _log_error(f"è§£ææç¤ºè¯æ–‡ä»¶ '{os.path.basename(file_path)}' å¤±è´¥: {e}ã€‚ä½¿ç”¨å†…ç½®é»˜è®¤æç¤ºè¯ã€‚")
        return default_built_in_prompts # ä¿®æ­£è¿™é‡Œï¼Œåº”è¯¥æ˜¯ default_built_in_prompts

# --- GLMæ–‡æœ¬å¯¹è¯èŠ‚ç‚¹ ---

class GLM_Text_Chat:
    """
    ä¸€ä¸ªç”¨äºåœ¨ ComfyUI ä¸­è°ƒç”¨æ™ºè°±AI GLM-4.5 æ¨¡å‹è¿›è¡Œæ–‡æœ¬èŠå¤©çš„èŠ‚ç‚¹ã€‚
    æ”¯æŒå¤šä¸ªé¢„è®¾ç³»ç»Ÿæç¤ºè¯ï¼ˆä»ç‰¹å®šæ ¼å¼çš„TXTæ–‡ä»¶åŠ è½½ï¼‰ï¼Œå¹¶æœ‰ä¼˜å…ˆçº§ç®¡ç†ã€‚
    èŠ‚ç‚¹ä¸­æš´éœ²äº†seedå‚æ•°ï¼Œå½“seedä¸º0æ—¶ï¼Œå†…éƒ¨ç”Ÿæˆéšæœºç§å­ã€‚
    """
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/X"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("response_text",)
    FUNCTION = "glm_chat_function"

    # å†…ç½®çš„é»˜è®¤ç³»ç»Ÿæç¤ºè¯ (å½“TXTæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥æ—¶ä½œä¸ºå¤‡ç”¨)
    _BUILT_IN_TEXT_PROMPTS = {
        "é»˜è®¤è§†é¢‘æ‰©å†™æç¤º (å†…ç½®)": """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘è„šæœ¬å’Œæç¤ºè¯ç”ŸæˆåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå°†å…¶æ‰©å†™æˆä¸€ä¸ªè¯¦ç»†ã€å…·ä½“ã€å¯Œæœ‰ç”»é¢æ„Ÿçš„è§†é¢‘ç”Ÿæˆæç¤ºè¯ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ç»“æ„å’Œè¦æ±‚ï¼š

1.  **ä¸»ä½“æè¿°ï¼š** è¯¦ç»†åˆ»ç”»è§†é¢‘ä¸­çš„ä¸»è¦å¯¹è±¡æˆ–äººç‰©çš„å¤–è§‚ã€ç‰¹å¾ã€çŠ¶æ€ã€‚
2.  **åœºæ™¯æè¿°ï¼š** ç»†è‡´æç»˜ä¸»ä½“æ‰€å¤„ç¯å¢ƒï¼ŒåŒ…æ‹¬æ—¶é—´ã€åœ°ç‚¹ã€èƒŒæ™¯å…ƒç´ ã€å…‰çº¿ã€å¤©æ°”ç­‰ã€‚
3.  **è¿åŠ¨æè¿°ï¼š** æ˜ç¡®ä¸»ä½“çš„åŠ¨ä½œç»†èŠ‚ï¼ˆå¹…åº¦ã€é€Ÿç‡ã€æ•ˆæœï¼‰ã€‚
4.  **é•œå¤´è¯­è¨€ï¼š** æŒ‡å®šæ™¯åˆ«ï¼ˆå¦‚ç‰¹å†™ã€è¿‘æ™¯ã€ä¸­æ™¯ã€å…¨æ™¯ï¼‰ã€è§†è§’ï¼ˆå¦‚å¹³è§†ã€ä»°è§†ã€ä¿¯è§†ï¼‰ã€é•œå¤´ç±»å‹ï¼ˆå¦‚å¹¿è§’ã€é•¿ç„¦ï¼‰ã€è¿é•œæ–¹å¼ï¼ˆå¦‚æ¨ã€æ‹‰ã€æ‘‡ã€ç§»ã€è·Ÿã€å‡ã€é™ï¼‰ã€‚
5.  **æ°›å›´è¯ï¼š** å®šä¹‰ç”»é¢çš„æƒ…æ„Ÿä¸æ°”æ°›ã€‚
6.  **é£æ ¼åŒ–ï¼š** è®¾å®šç”»é¢çš„è‰ºæœ¯é£æ ¼ï¼ˆå¦‚å†™å®ã€å¡é€šã€èµ›åšæœ‹å…‹ã€æ°´å¢¨ç”»ã€ç”µå½±æ„Ÿã€æŠ½è±¡ï¼‰ã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
-   åªè¾“å‡ºæœ€ç»ˆæ‰©å†™åçš„è§†é¢‘æç¤ºè¯ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—æˆ–é¢å¤–çš„å¯¹è¯ã€‚
-   å°†æ‰€æœ‰è¦ç´ èåˆä¸ºä¸€æ®µè¿è´¯çš„æè¿°æ€§æ–‡å­—ï¼Œç¡®ä¿é€»è¾‘æµç•…ã€‚
-   æœ€ç»ˆæç¤ºè¯åº”è¯¥å°½å¯èƒ½è¯¦ç»†ï¼ŒåŒ…å«ä¸°å¯Œçš„ç»†èŠ‚ï¼Œä»¥ä¾¿AIæ¨¡å‹èƒ½å‡†ç¡®ç†è§£å¹¶ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ã€‚

**ä¸¾ä¾‹ï¼š**
ç”¨æˆ·è¾“å…¥ï¼šä¸€åªå°ç‹—åœ¨è‰åœ°ä¸Šç©è€ã€‚
ä½ çš„è¾“å‡ºï¼šä¸€åªæ¯›èŒ¸èŒ¸çš„é‡‘æ¯›å¹¼çŠ¬ï¼ŒæŠ«ç€é˜³å…‰èˆ¬é‡‘è‰²çš„æ¯›å‘ï¼Œçœ¼ç¥å¥½å¥‡è€Œæ´»æ³¼ï¼Œåœ¨é˜³å…‰æ˜åªšçš„å¹¿é˜”è‰åœ°ä¸Šå¥”è·‘ã€‚å®ƒæ¬¢å¿«åœ°è¿½é€ç€ä¸€åªé£èˆçš„è´è¶ï¼Œæ—¶è€Œè·³è·ƒï¼Œæ—¶è€Œæ‰“æ»šï¼Œè‰å±‘å’Œæ³¥åœŸæº…èµ·ç»†å°çš„å¼§çº¿ã€‚ä¸­æ™¯ï¼Œä½è§’åº¦ä»°æ‹ï¼Œé•œå¤´éšç€å°ç‹—çš„å¥”è·‘è€Œå¹³ç¨³åœ°æ¨ªå‘ç§»åŠ¨ï¼Œå±•ç°å‡ºè‰åœ°çš„å¹¿é˜”å’Œå°ç‹—çš„æ´»åŠ›ã€‚ç”»é¢å……æ»¡æ¸©æš–ã€å¿«ä¹ã€ç”Ÿæœºå‹ƒå‹ƒçš„æ°›å›´ï¼Œè‰²å½©é²œè‰³ï¼Œå¦‚ç”°å›­è¯—èˆ¬çš„å¡é€šé£æ ¼ã€‚
"""
    }

    @classmethod
    def get_text_prompts(cls):
        """åŠ è½½å¤–éƒ¨æˆ–å†…ç½®çš„æ–‡æœ¬æç¤ºè¯å­—å…¸ã€‚"""
        # å°è¯•ä»å¤–éƒ¨TXTæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°å†…ç½®é»˜è®¤
        return load_prompts_from_txt(
            os.path.join(CURRENT_DIR, TEXT_PROMPTS_FILE_NAME),
            cls._BUILT_IN_TEXT_PROMPTS
        )

    @classmethod
    def INPUT_TYPES(s):
        available_prompts = s.get_text_prompts()
        prompt_keys = list(available_prompts.keys())
        default_selection = prompt_keys[0] if prompt_keys else "æ— å¯ç”¨æç¤ºè¯"

        return {
            "required": {
                "text_system_prompt_preset": (prompt_keys, {"default": default_selection}),
                "system_prompt_override": ("STRING", {"multiline": True, "default": "", "placeholder": "ç³»ç»Ÿæç¤ºè¯ (æœ€é«˜ä¼˜å…ˆçº§ï¼Œç•™ç©ºåˆ™ä»é¢„è®¾åŠ è½½)"}),
                "api_key": ("STRING", {"default": "", "multiline": False, "placeholder": "å¯é€‰ï¼šæ™ºè°±AI API Key (ç•™ç©ºåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–config.jsonè¯»å–)"}),
                "model_name": ("STRING", {"default": "glm-4.5v", "placeholder": "è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ glm-4.5v"}),
                "temperature": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_p": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.01}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "è®¾ç½®ä¸º0æ—¶ï¼Œæ¯æ¬¡è¿è¡Œç”Ÿæˆéšæœºç§å­ï¼›è®¾ç½®ä¸ºå…¶ä»–å€¼æ—¶ï¼Œä½¿ç”¨å›ºå®šç§å­ã€‚æ³¨æ„ï¼šæ­¤ç§å­ä»…å½±å“ComfyUIèŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ•°ç”Ÿæˆï¼Œä¸ç›´æ¥å½±å“æ™ºè°±AIæ¨¡å‹çš„è¾“å‡ºç»“æœã€‚"}),
                "text_input": ("STRING", {"multiline": True, "default": "è¯·æ‰©å†™å…³äºä¸€åªå°ç‹—åœ¨è‰åœ°ä¸Šç©è€çš„è§†é¢‘æç¤ºè¯ã€‚", "placeholder": "è¯·è¾“å…¥éœ€è¦æ‰©å†™çš„è§†é¢‘æç¤ºè¯å†…å®¹"}),
            }
        }

    def glm_chat_function(self, text_input, api_key, model_name, temperature, top_p, max_tokens, seed, system_prompt_override, text_system_prompt_preset):
        """
        æ‰§è¡Œæ™ºè°±AI GLM-4.5 æ–‡æœ¬èŠå¤©åŠŸèƒ½ã€‚
        """
        final_api_key = api_key.strip() or get_zhipuai_api_key()
        if not final_api_key:
            _log_error("API Key æœªæä¾›ã€‚")
            return ("API Key æœªæä¾›ã€‚",)

        _log_info("åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯ã€‚")

        try:
            client = ZhipuAI(api_key=final_api_key)
        except Exception as e:
            _log_error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return (f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}",)

        # --- ç³»ç»Ÿæç¤ºè¯ç¡®å®šä¼˜å…ˆçº§ ---
        final_system_prompt = ""
        available_prompts = self.get_text_prompts()

        if system_prompt_override and system_prompt_override.strip():
            final_system_prompt = system_prompt_override.strip()
            _log_info("ä½¿ç”¨ 'system_prompt_override'ã€‚")
        elif text_system_prompt_preset in available_prompts:
            final_system_prompt = available_prompts[text_system_prompt_preset]
            _log_info(f"ä½¿ç”¨é¢„è®¾æç¤ºè¯: '{text_system_prompt_preset}'ã€‚")
        else:
            if available_prompts:
                final_system_prompt = list(available_prompts.values())[0]
                _log_warning(f"é¢„è®¾ '{text_system_prompt_preset}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é¢„è®¾ã€‚")
            else:
                final_system_prompt = list(self._BUILT_IN_TEXT_PROMPTS.values())[0]
                _log_warning("æ— å¯ç”¨é¢„è®¾æç¤ºè¯ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨ã€‚")


        if not final_system_prompt:
            _log_error("ç³»ç»Ÿæç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚")
            return ("ç³»ç»Ÿæç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚",)

        # ç¡®ä¿ final_system_prompt ç¡®å®æ˜¯å­—ç¬¦ä¸²
        if not isinstance(final_system_prompt, str):
            _log_warning(f"ç³»ç»Ÿæç¤ºè¯ç±»å‹å¼‚å¸¸: {type(final_system_prompt)}ã€‚å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚")
            final_system_prompt = str(final_system_prompt)

        messages = [
            {"role": "system", "content": final_system_prompt},
            {"role": "user", "content": text_input}
        ]

        # --- ç§å­é€»è¾‘ ---
        effective_seed = seed if seed != 0 else random.randint(0, 0xffffffffffffffff)
        _log_info(f"å†…éƒ¨ç§å­: {effective_seed}ã€‚")
        random.seed(effective_seed) # ä»…å½±å“èŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ€§ï¼Œå¦‚æœªæ¥å¯èƒ½æ‰©å±•çš„éšæœºé€‰æ‹©é€»è¾‘

        _log_info(f"è°ƒç”¨ GLM-4.5 ({model_name})...")

        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )
            response_text = response.choices[0].message.content
            _log_info("GLM-4.5 å“åº”æˆåŠŸã€‚")
            return (response_text,)
        except Exception as e:
            error_message = f"GLM-4.5 API è°ƒç”¨å¤±è´¥: {e}"
            return (error_message,)

# --- GLMè¯†å›¾ç”Ÿæˆæç¤ºè¯èŠ‚ç‚¹ ---

class GLM_Vision_ImageToPrompt:
    """
    ä¸€ä¸ªç”¨äºåœ¨ ComfyUI ä¸­è°ƒç”¨æ™ºè°±AI GLM-4.5V æ¨¡å‹ï¼Œ
    æ ¹æ®å›¾ç‰‡ URLã€Base64 ç¼–ç çš„å›¾ç‰‡æ•°æ®æˆ–ç›´æ¥çš„ComfyUI IMAGEå¯¹è±¡ç”Ÿæˆå›¾ç‰‡æè¿°æç¤ºè¯çš„èŠ‚ç‚¹ã€‚
    æ”¯æŒå¤šä¸ªé¢„è®¾è¯†å›¾æç¤ºè¯ï¼ˆä»ç‰¹å®šæ ¼å¼çš„TXTæ–‡ä»¶åŠ è½½ï¼‰ï¼Œå¹¶æœ‰ä¼˜å…ˆçº§ç®¡ç†ã€‚
    """
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/X"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("GETPrompt",)
    FUNCTION = "generate_prompt"

    # å†…ç½®çš„é»˜è®¤è¯†å›¾æç¤ºè¯ (å½“TXTæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥æ—¶ä½œä¸ºå¤‡ç”¨)
    _BUILT_IN_IMAGE_PROMPTS = {
        "é€šç”¨é«˜è´¨é‡è‹±æ–‡æè¿° (å†…ç½®)": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒæè¿°ä¸“å®¶ï¼Œèƒ½å¤Ÿå°†å›¾ç‰‡å†…å®¹è½¬åŒ–ä¸ºé«˜è´¨é‡çš„è‹±æ–‡æç¤ºè¯ï¼Œç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ã€‚è¯·ä»”ç»†è§‚å¯Ÿæä¾›çš„å›¾ç‰‡ï¼Œå¹¶ç”Ÿæˆä¸€æ®µè¯¦ç»†ã€å…·ä½“ã€å¯Œæœ‰åˆ›é€ æ€§çš„è‹±æ–‡çŸ­è¯­ï¼Œæè¿°å›¾ç‰‡ä¸­çš„ä¸»ä½“å¯¹è±¡ã€åœºæ™¯ã€åŠ¨ä½œã€å…‰çº¿ã€æè´¨ã€è‰²å½©ã€æ„å›¾å’Œè‰ºæœ¯é£æ ¼ã€‚è¦æ±‚ï¼šè¯­è¨€ï¼šä¸¥æ ¼ä½¿ç”¨è‹±æ–‡ã€‚ç»†èŠ‚ï¼šå°½å¯èƒ½å¤šåœ°æç»˜å›¾ç‰‡ç»†èŠ‚ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç‰©ä½“ã€äººç‰©ã€èƒŒæ™¯ã€å‰æ™¯ã€çº¹ç†ã€è¡¨æƒ…ã€åŠ¨ä½œã€æœè£…ã€é“å…·ç­‰ã€‚è§’åº¦ï¼šå°½å¯èƒ½ä»å¤šä¸ªè§’åº¦ä¸°å¯Œæè¿°ï¼Œä¾‹å¦‚ç‰¹å†™ã€å¹¿è§’ã€ä¿¯è§†ã€ä»°è§†ç­‰ï¼Œä½†ä¸è¦ç›´æ¥å†™â€œè§’åº¦â€ã€‚è¿æ¥ï¼šä½¿ç”¨é€—å·ï¼ˆ,ï¼‰è¿æ¥ä¸åŒçš„çŸ­è¯­ï¼Œå½¢æˆä¸€ä¸ªè¿è´¯çš„æç¤ºè¯ã€‚äººç‰©ï¼šæç»˜äººç‰©æ—¶ï¼Œä½¿ç”¨ç¬¬ä¸‰äººç§°ï¼ˆå¦‚ 'a woman', 'the man'ï¼‰ã€‚è´¨é‡è¯ï¼šåœ¨ç”Ÿæˆçš„æç¤ºè¯æœ«å°¾ï¼ŒåŠ¡å¿…æ·»åŠ ä»¥ä¸‹è´¨é‡å¢å¼ºè¯ï¼š', best quality, high resolution, 4k, high quality, masterpiece, photorealistic'"
    }

    @classmethod
    def get_image_prompts(cls):
        """åŠ è½½å¤–éƒ¨æˆ–å†…ç½®çš„å›¾åƒæç¤ºè¯å­—å…¸ã€‚"""
        return load_prompts_from_txt(
            os.path.join(CURRENT_DIR, IMAGE_PROMPTS_FILE_NAME),
            cls._BUILT_IN_IMAGE_PROMPTS
        )

    @classmethod
    def INPUT_TYPES(cls):
        available_prompts = cls.get_image_prompts()
        prompt_keys = list(available_prompts.keys())
        default_selection = prompt_keys[0] if prompt_keys else "æ— å¯ç”¨æç¤ºè¯"

        return {
            "required": {
                "image_prompt_preset": (prompt_keys, {"default": default_selection}),
                "prompt_override": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è¯·è¾“å…¥ç”¨äºæè¿°å›¾ç‰‡çš„æ–‡æœ¬æç¤ºè¯ (æœ€é«˜ä¼˜å…ˆçº§ï¼Œç•™ç©ºåˆ™ä»é¢„è®¾åŠ è½½)"
                }),
                "model_name": ("STRING", {
                    "default": "glm-4.5v",
                    "placeholder": "è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ glm-4.5v"
                }),
                "api_key":  ("STRING", {
                    "multiline": False,
                    "default": "",
                    "placeholder": "å¯é€‰ï¼šæ™ºè°±AI API Key (ç•™ç©ºåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–config.jsonè¯»å–)"
                }),
                "seed": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 0xffffffffffffffff,
                    "step": 1,
                    "display": "number",
                    "tooltip": "è®¾ç½®ä¸º0æ—¶ï¼Œæ¯æ¬¡è¿è¡Œç”Ÿæˆéšæœºç§å­ï¼›è®¾ç½®ä¸ºå…¶ä»–å€¼æ—¶ï¼Œä½¿ç”¨å›ºå®šç§å­ã€‚æ³¨æ„ï¼šæ­¤ç§å­ä»…å½±å“ComfyUIèŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ•°ç”Ÿæˆï¼Œä¸ç›´æ¥å½±å“æ™ºè°±AIæ¨¡å‹çš„è¾“å‡ºç»“æœã€‚"
                }),
            },
            "optional": {
                "image_url": ("STRING", {
                    "default": "",
                    "placeholder": "è¯·è¾“å…¥å›¾ç‰‡URL (ä¸Base64/IMAGEä¸‰é€‰ä¸€)"
                }),
                "image_base64": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": "è¯·è¾“å…¥Base64ç¼–ç çš„å›¾ç‰‡æ•°æ® (ä¸URL/IMAGEä¸‰é€‰ä¸€)"
                }),
                "image_input": ("IMAGE", {"optional": True, "tooltip": "ç›´æ¥è¾“å…¥ComfyUI IMAGEå¯¹è±¡ (ä¸URL/Base64ä¸‰é€‰ä¸€)"}), # æ–°å¢IMAGEè¾“å…¥
            }
        }

    def generate_prompt(self, api_key, prompt_override, model_name, seed, image_url="", image_base64="", image_prompt_preset="", image_input=None):
        """
        æ‰§è¡Œæ™ºè°±AI GLM-4.5V è¯†å›¾ç”Ÿæˆæç¤ºè¯åŠŸèƒ½ã€‚
        """
        final_api_key = api_key.strip() or get_zhipuai_api_key()
        if not final_api_key:
            _log_error("API Key æœªæä¾›ã€‚")
            return ("API Key æœªæä¾›ã€‚",)
        _log_info("åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯ã€‚")

        try:
            client = ZhipuAI(api_key=final_api_key)
        except Exception as e:
            _log_error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return (f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}",)

        # --- è¾“å…¥æ ¡éªŒï¼šå›¾ç‰‡URLã€Base64å›¾ç‰‡æˆ–IMAGEå¯¹è±¡è‡³å°‘æä¾›ä¸€ä¸ª ---
        image_url_provided = bool(image_url and image_url.strip())
        image_base64_provided = bool(image_base64 and image_base64.strip())
        image_input_provided = image_input is not None

        if not (image_url_provided or image_base64_provided or image_input_provided):
            _log_error("å¿…é¡»æä¾›å›¾ç‰‡URLã€Base64æ•°æ®æˆ–IMAGEå¯¹è±¡ã€‚")
            return ("å¿…é¡»æä¾›å›¾ç‰‡URLã€Base64æ•°æ®æˆ–IMAGEå¯¹è±¡ã€‚",)

        # --- å¤„ç†å›¾ç‰‡è¾“å…¥ä¼˜å…ˆçº§ï¼šIMAGE > Base64 > URL ---
        final_image_data = None
        if image_input_provided:
            _log_info("æ£€æµ‹åˆ° IMAGE å¯¹è±¡è¾“å…¥ï¼Œæ­£åœ¨è½¬æ¢ä¸º Base64ã€‚")
            try:
                # ComfyUIçš„IMAGEæ˜¯PyTorchå¼ é‡ï¼ŒèŒƒå›´[0,1]ï¼Œå½¢çŠ¶[B, H, W, C]
                # è½¬æ¢ä¸ºPIL Imageï¼ŒèŒƒå›´[0,255]ï¼Œå½¢çŠ¶[H, W, C]
                i = 255. * image_input.cpu().numpy()
                img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8)[0]) # å–ç¬¬ä¸€ä¸ªbatchçš„å›¾ç‰‡
                
                buffered = io.BytesIO()
                img.save(buffered, format="PNG") # é€šå¸¸PNGæ˜¯æ— æŸä¸”æ”¯æŒé€æ˜åº¦
                final_image_data = "data:image/png;base64," + base64.b64encode(buffered.getvalue()).decode('utf-8')
                _log_info("IMAGE å¯¹è±¡æˆåŠŸè½¬æ¢ä¸º Base64ã€‚")
            except Exception as e:
                _log_error(f"å°† IMAGE å¯¹è±¡è½¬æ¢ä¸º Base64 å¤±è´¥: {e}")
                return (f"å°† IMAGE å¯¹è±¡è½¬æ¢ä¸º Base64 å¤±è´¥: {e}",)
        elif image_base64_provided:
            _log_info("æ£€æµ‹åˆ° Base64 å­—ç¬¦ä¸²è¾“å…¥ã€‚")
            if image_base64.startswith("data:image/"):
                final_image_data = image_base64
            else:
                _log_warning("Base64å­—ç¬¦ä¸²ç¼ºå°‘å‰ç¼€ï¼Œå°è¯•æ·»åŠ é»˜è®¤JPEGå‰ç¼€ã€‚")
                try:
                    # å°è¯•è§£ç éªŒè¯æœ‰æ•ˆæ€§ï¼Œå¹¶æ·»åŠ å¸¸è§å‰ç¼€
                    base64.b64decode(image_base64.split(',')[-1])
                    final_image_data = f"data:image/jpeg;base64,{image_base64}"
                except Exception as decode_e:
                    _log_error(f"Base64è§£ç å¤±è´¥: {decode_e}")
                    return ("æä¾›çš„Base64å›¾ç‰‡æ•°æ®æ— æ•ˆã€‚",)
        elif image_url_provided:
            _log_info(f"æ£€æµ‹åˆ°å›¾ç‰‡URLè¾“å…¥: {image_url}")
            final_image_data = image_url

        if not final_image_data:
            _log_error("æœªèƒ½è·å–æœ‰æ•ˆçš„å›¾ç‰‡æ•°æ®ã€‚")
            return ("æœªèƒ½è·å–æœ‰æ•ˆçš„å›¾ç‰‡æ•°æ®ã€‚",)

        # --- è¯†å›¾æç¤ºè¯ç¡®å®šä¼˜å…ˆçº§ ---
        final_prompt_text = ""
        available_prompts = self.get_image_prompts()

        if prompt_override and prompt_override.strip():
            final_prompt_text = prompt_override.strip()
            _log_info("ä½¿ç”¨ 'prompt_override'ã€‚")
        elif image_prompt_preset in available_prompts:
            final_prompt_text = available_prompts[image_prompt_preset]
            _log_info(f"ä½¿ç”¨é¢„è®¾è¯†å›¾æç¤ºè¯: '{image_prompt_preset}'ã€‚")
        else:
            if available_prompts:
                final_prompt_text = list(available_prompts.values())[0]
                _log_warning(f"é¢„è®¾ '{image_prompt_preset}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é¢„è®¾ã€‚")
            else:
                final_prompt_text = list(self._BUILT_IN_IMAGE_PROMPTS.values())[0]
                _log_warning("æ— å¯ç”¨é¢„è®¾è¯†å›¾æç¤ºè¯ï¼Œä½¿ç”¨å†…ç½®å¤‡ç”¨ã€‚")


        if not final_prompt_text:
            _log_error("è¯†å›¾æç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚")
            return ("è¯†å›¾æç¤ºè¯ä¸èƒ½ä¸ºç©ºã€‚",)

        # ç¡®ä¿ final_prompt_text ç¡®å®æ˜¯å­—ç¬¦ä¸²
        if not isinstance(final_prompt_text, str):
            _log_warning(f"è¯†å›¾æç¤ºè¯ç±»å‹å¼‚å¸¸: {type(final_prompt_text)}ã€‚å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚")
            final_prompt_text = str(final_prompt_text)

        # --- æ„å»ºæ¶ˆæ¯å†…å®¹ ---
        content_parts = [{"type": "text", "text": final_prompt_text}]
        content_parts.append({"type": "image_url", "image_url": {"url": final_image_data}})


        # --- ç§å­é€»è¾‘ (æ™ºè°±AI GLM-4.5V APIé€šå¸¸ä¸æ”¯æŒç›´æ¥çš„seedå‚æ•°ï¼Œæ­¤å‚æ•°ä»…ç”¨äºComfyUIèŠ‚ç‚¹å†…éƒ¨) ---
        effective_seed = seed if seed != 0 else random.randint(0, 0xffffffffffffffff)
        _log_info(f"å†…éƒ¨ç§å­: {effective_seed}ã€‚")
        random.seed(effective_seed) # ä»…å½±å“èŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ€§

        _log_info(f"è°ƒç”¨ GLM-4.5V ({model_name})...")
    
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": content_parts}]
            )
            response_content = str(response.choices[0].message.content)
            _log_info("GLM-4.5V å“åº”æˆåŠŸã€‚")
            return (response_content,)
        except Exception as e:
            error_message = f"GLM-4.5V API è°ƒç”¨å¤±è´¥: {e}"
            _log_error(error_message)
            return (error_message,)

# --- GLMæ–‡æœ¬ç¿»è¯‘èŠ‚ç‚¹ ---

class GLM_Translation_Text:
    """
    ä¸€ä¸ªç”¨äºåœ¨ ComfyUI ä¸­è°ƒç”¨æ™ºè°±AI GLMæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç¿»è¯‘çš„èŠ‚ç‚¹ã€‚
    """
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/X"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("translated_text",)
    FUNCTION = "glm_translate_function"

    @classmethod
    def INPUT_TYPES(s):
        # å°è¯•ä»config.jsonåŠ è½½é»˜è®¤ç¿»è¯‘è¯­è¨€
        config_path = os.path.join(CURRENT_DIR, CONFIG_FILE_NAME)
        default_from_lang = "zh"
        default_to_lang = "en"
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                default_from_lang = config.get("from_translate", default_from_lang)
                default_to_lang = config.get("to_translate", default_to_lang)
        except Exception as e:
            _log_warning(f"åŠ è½½ç¿»è¯‘é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤è¯­è¨€ã€‚")

        # ç¡®ä¿é»˜è®¤è¯­è¨€åœ¨SUPPORTED_TRANSLATION_LANGSä¸­
        if default_from_lang not in SUPPORTED_TRANSLATION_LANGS:
            default_from_lang = "zh"
        if default_to_lang not in SUPPORTED_TRANSLATION_LANGS:
            default_to_lang = "en"

        return {
            "required": {
                "text_input": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œä¸–ç•Œï¼", "placeholder": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬"}),
                "from_language": (SUPPORTED_TRANSLATION_LANGS, {"default": default_from_lang, "tooltip": "æºè¯­è¨€"}),
                "to_language": (SUPPORTED_TRANSLATION_LANGS, {"default": default_to_lang, "tooltip": "ç›®æ ‡è¯­è¨€"}),
                "api_key": ("STRING", {"default": "", "multiline": False, "placeholder": "å¯é€‰ï¼šæ™ºè°±AI API Key (ç•™ç©ºåˆ™å°è¯•ä»ç¯å¢ƒå˜é‡æˆ–config.jsonè¯»å–)"}),
                "model_name": ("STRING", {"default": "glm-4.1v-thinking-flashx", "placeholder": "è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œå¦‚ glm-4.5v"}),
                "temperature": ("FLOAT", {"default": 0.1, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "ç¿»è¯‘ä»»åŠ¡å»ºè®®è¾ƒä½çš„æ¸©åº¦å€¼ä»¥ä¿æŒå‡†ç¡®æ€§"}),
                "top_p": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.0, "step": 0.01}),
                "max_tokens": ("INT", {"default": 1024, "min": 1, "max": 4096}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "è®¾ç½®ä¸º0æ—¶ï¼Œæ¯æ¬¡è¿è¡Œç”Ÿæˆéšæœºç§å­ï¼›è®¾ç½®ä¸ºå…¶ä»–å€¼æ—¶ï¼Œä½¿ç”¨å›ºå®šç§å­ã€‚æ³¨æ„ï¼šæ­¤ç§å­ä»…å½±å“ComfyUIèŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ•°ç”Ÿæˆï¼Œä¸ç›´æ¥å½±å“æ™ºè°±AIæ¨¡å‹çš„è¾“å‡ºç»“æœã€‚"}),
            }
        }

    def glm_translate_function(self, text_input, from_language, to_language, api_key, model_name, temperature, top_p, max_tokens, seed):
        """
        æ‰§è¡Œæ™ºè°±AI GLMæ–‡æœ¬ç¿»è¯‘åŠŸèƒ½ã€‚
        """
        final_api_key = api_key.strip() or get_zhipuai_api_key()
        if not final_api_key:
            _log_error("API Key æœªæä¾›ã€‚")
            return ("API Key æœªæä¾›ã€‚",)

        _log_info("åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯ã€‚")
        try:
            client = ZhipuAI(api_key=final_api_key)
        except Exception as e:
            _log_error(f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return (f"å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}",)

        if not text_input or not text_input.strip():
            _log_warning("è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œä¸è¿›è¡Œç¿»è¯‘ã€‚")
            return ("",)

        # æ„å»ºç¿»è¯‘ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·è¾“å…¥
        # æ™ºè°±AIæœ¬èº«å¯èƒ½æ²¡æœ‰ä¸“é—¨çš„ç¿»è¯‘APIï¼Œé€šå¸¸é€šè¿‡æŒ‡ä»¤LLMæ¥å®Œæˆ
        system_prompt = f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„æ–‡æœ¬ä»{from_language}ç¿»è¯‘æˆ{to_language}ã€‚åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚"
        user_message = text_input

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]

        # --- ç§å­é€»è¾‘ ---
        effective_seed = seed if seed != 0 else random.randint(0, 0xffffffffffffffff)
        _log_info(f"å†…éƒ¨ç§å­: {effective_seed}ã€‚")
        random.seed(effective_seed) # ä»…å½±å“èŠ‚ç‚¹å†…éƒ¨çš„éšæœºæ€§

        _log_info(f"è°ƒç”¨ GLM ({model_name}) è¿›è¡Œç¿»è¯‘...")
        _log_info(f"  ä» '{from_language}' ç¿»è¯‘åˆ° '{to_language}'ã€‚")

        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
            )
            translated_text = response.choices[0].message.content
            _log_info("GLM ç¿»è¯‘å“åº”æˆåŠŸã€‚")
            return (translated_text,)
        except Exception as e:
            error_message = f"GLM API ç¿»è¯‘è°ƒç”¨å¤±è´¥: {e}"
            _log_error(error_message)
            return (error_message,)
