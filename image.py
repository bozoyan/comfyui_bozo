import os
import json
import math  # æ·»åŠ  math æ¨¡å—å¯¼å…¥
from pathlib import Path
from nodes import SaveImage
import folder_paths
import requests
from PIL import Image
from io import BytesIO
import numpy as np
import time
import torch
import torch.nn.functional as F
import urllib.request
import cv2

# å¿…è¦çš„å¸¸é‡å®šä¹‰
BOOLEAN = ("BOOLEAN", {"default": True})
CONFIG = {"indent": 2}

# ç®€åŒ–çš„æ—¥å¿—å¤„ç†
class Logger:
    @staticmethod
    def debug(msg): print(f"[DEBUG] {msg}")
    @staticmethod
    def error(msg): print(f"[ERROR] {msg}")

logger = Logger()

class BImageSaveWithExtraMetadata(SaveImage):
    def __init__(self):
        super().__init__()
        self.data_cached = None
        self.data_cached_text = None

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "filename_prefix": ("STRING", {"default": "ComfyUI"}),
                "with_workflow": BOOLEAN,
            },
            "optional": {
                "metadata_extra": ("STRING", {"multiline": True, "default": json.dumps({
                  "Title": "Image generated by \u56b4\u6ce2\u8a2d\u8a08",
                  "Description": "More info: https://slas.cc",
                  "Author": "BOZOYAN",
                  "Software": "ComfyUI",
                  "Category": "StableDiffusion",
                  "Rating": 5,
                  "UserComment": "",
                  "Keywords": [""],
                  "Copyrights": "BOZO DESIGN",
                }, indent=CONFIG["indent"]).replace("\\/", "/"),
                }),
            },
            "hidden": {
                "prompt": "PROMPT",
                "extra_pnginfo": "EXTRA_PNGINFO",
            },
        }

    RETURN_TYPES = ("METADATA_RAW",)
    RETURN_NAMES = ("Metadata RAW",)
    OUTPUT_NODE = True
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"
    OUTPUT_NODE = True
    FUNCTION = "execute"

    def execute(self, image=None, filename_prefix="ComfyUI", with_workflow=True, metadata_extra=None, prompt=None, extra_pnginfo=None):
        data = {
            "result": [''],
            "ui": {
                "text": [''],
                "images": [],
            }
        }
        if image is not None:
            if with_workflow is True:
                extra_pnginfo_new = extra_pnginfo.copy()
                prompt = prompt.copy()
            else:
                extra_pnginfo_new = None
                prompt = None

            if metadata_extra is not None and metadata_extra != 'undefined':
                try:
                    metadata_extra = json.loads(metadata_extra)
                except Exception as e:
                    logger.error(f"Error parsing metadata_extra (it will send as string), error: {e}")
                    metadata_extra = {"extra": str(metadata_extra)}

                if isinstance(metadata_extra, dict):
                    for k, v in metadata_extra.items():
                        if extra_pnginfo_new is None:
                            extra_pnginfo_new = {}
                        extra_pnginfo_new[k] = v

            saved = super().save_images(image, filename_prefix, prompt, extra_pnginfo_new)
            data["ui"]["images"] = saved["ui"]["images"]

        else:
            logger.debug("Source: Empty on BImageSaveWithExtraMetadata")

        return data


class BImageYunSuan:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "width": ("INT", {"default": 1280, "min": 1, "max": 10240}),
                "height": ("INT", {"default": 1280, "min": 1, "max": 10240}),
                "formula": ("STRING", {
                    "multiline": True, 
                    "default": "math.sqrt((1280 * 1280) / (width * height))"
                }),
            },
        }

    RETURN_TYPES = ("STRING", "INT", "FLOAT")
    RETURN_NAMES = ("result", "result_int", "result_float")
    FUNCTION = "execute"
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"

    def execute(self, width, height, formula):
        try:
            # åˆ›å»ºæœ¬åœ°å˜é‡ä¾›å…¬å¼ä½¿ç”¨
            locals_dict = {"math": math, "width": width, "height": height}
            # æ‰§è¡Œå…¬å¼è®¡ç®—
            result = eval(formula, {"__builtins__": {}}, locals_dict)
            # å°†ç»“æœè½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œä¿ç•™1ä½å°æ•°ï¼Œä¸å››èˆäº”å…¥ ä¿®æ”¹å°¾éƒ¨:1 æ•°å€¼ä¿ç•™ä¸€ä½å°æ•°
            result_str = str(float(str(result).split('.')[0] + '.' + str(result).split('.')[1][:1])) if '.' in str(result) else str(result)
            
            # è½¬æ¢ä¸ºæ•´æ•°
            result_int = int(float(result_str))
            
            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œä¿ç•™1ä½å°æ•°
            result_float = float(result_str)
            
            return (result_str, result_int, result_float)
        except Exception as e:
            logger.error(f"è®¡ç®—å…¬å¼é”™è¯¯: {e}")
            return ("", 0, 0.0)


class PicRun:
    def __init__(self):
        try:
            # ä½¿ç”¨ os.path.join æ„å»ºè·¯å¾„ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
            key_folder = os.path.join(os.path.dirname(__file__), 'key')
            # ç¡®ä¿ key æ–‡ä»¶å¤¹å­˜åœ¨
            if not os.path.exists(key_folder):
                os.makedirs(key_folder, exist_ok=True)
                logger.error(f"åˆ›å»º key æ–‡ä»¶å¤¹: {key_folder}")
                
            api_key_path = os.path.join(key_folder, 'modelscope_api_key.txt')
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(api_key_path):
                logger.error(f"API key æ–‡ä»¶ä¸å­˜åœ¨: {api_key_path}")
                self.api_key = None
            else:
                with open(api_key_path, 'r') as f:
                    self.api_key = f.read().strip()
                if not self.api_key:
                    logger.error("API key æ–‡ä»¶ä¸ºç©º")
                    self.api_key = None
        except Exception as e:
            logger.error(f"è¯»å– API key æ–‡ä»¶å¤±è´¥: {str(e)}")
            self.api_key = None
        
        self.size_presets = {
            "1:1 (1240x1240)": (1240, 1240),
            "4:3 (1080x1440)": (1080, 1440),
            "3:4 (1440x1080)": (1440, 1080),
            "1:2 (872x1744)": (872, 1744),
            "2:1 (1744x872)": (1744, 872),
            "2:3 (960x1440)": (960, 1440),
            "3:2 (1440x960)": (1440, 960),
            "2:5 (720x1800)": (720, 1800),
            "5:2 (1800x720)": (1800, 720),
            "3:5 (960x1600)": (960, 1600),
            "5:3 (1600x960)": (1600, 960),
            "4:5 (1080x1350)": (1080, 1350),
            "5:4 (1350x1080)": (1350, 1080),
            "9:16 (900x1600)": (900, 1600),
            "16:9 (1600x900)": (1600, 900),
        }

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("STRING", {"default": "bozoyan/F_feifei"}),
                "prompt": ("STRING", {
                    "multiline": True,
                    "default": "A woman in a purple lace bra and pant is sitting on a bed."
                }),
                "steps": ("INT", {"default": 20, "min": 8, "max": 100}),
                "guidance": ("FLOAT", {"default": 3.5, "min": 1.5, "max": 20.0}),
                "sampler": (["DPM++ SDE", "DPM++ SDE Karras", "DPM++ 2M SDE", "DPM++ 2M Karras", "LCM", "DPM2 a", "Euler a", "Euler", "DDIM Normal"], {"default": "Euler"}),
                "size_preset": (["è‡ªå®šä¹‰", "1:1 (1240x1240)", "4:3 (1080x1440)", "3:4 (1440x1080)",
                "1:2 (872x1744)", "2:1 (1744x872)",
                "2:3 (960x1440)", "3:2 (1440x960)", "2:5 (720x1800)",
                "5:2 (1800x720)", "3:5 (960x1600)", "5:3 (1600x960)",
                "4:5 (1080x1350)", "5:4 (1350x1080)", "9:16 (900x1600)",
                "16:9 (1600x900)"], ),
                "width": ("INT", {"default": 1080, "min": 64, "max": 2400}),
                "height": ("INT", {"default": 1440, "min": 64, "max": 2400}),
                "seed": ("INT", {"default": 42, "min": 0, "max": 2147483647}),
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING", "STRING", "INT", "FLOAT", "STRING", "INT", "INT", "INT", "STRING")
    RETURN_NAMES = ("image", "image_url", "model", "steps", "guidance", "sampler", "width", "height", "seed", "prompt")
    FUNCTION = "execute"
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/X"

    def _download_image(self, url):
        """ä»URLä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºtensor"""
        try:
            response = urllib.request.urlopen(url)
            image_data = response.read()
            image = Image.open(BytesIO(image_data)).convert('RGB')
            
            # è½¬æ¢ä¸ºtensoræ ¼å¼ [H, W, 3]
            img_tensor = torch.tensor(list(image.getdata())).reshape(image.height, image.width, 3).float() / 255.0
            
            # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ [1, H, W, 3]
            return img_tensor.unsqueeze(0)
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾åƒæ—¶å‡ºé”™: {e}")
            return None

    def execute(self, model, prompt, steps, guidance, sampler, size_preset, width, height, seed):
        # æ‰“å°å…³é”®ä¿¡æ¯
        print("=" * 80)
        print(f"æ‰€å±æ¨¡å‹: {model} é‡‡æ ·ï¼š{sampler} CFG:{guidance} æ­¥æ•°: {steps} å›¾ç‰‡å°ºå¯¸: {size_preset}")
        print(f"æç¤ºè¯: {prompt}")
        print("=" * 80)

        if self.api_key is None:
            logger.error("æ— æ³•è¯»å– API key")
            # è¿”å›ç©ºå›¾åƒå’Œé”™è¯¯ä¿¡æ¯
            empty_image = torch.zeros((1, 64, 64, 3))
            return (empty_image, "é”™è¯¯: æ— æ³•ä» modelscope_api_key.txt è¯»å–æœ‰æ•ˆçš„ token", "", 0, 0.0, "", 0, 0, 0, "")
            
        if size_preset != "è‡ªå®šä¹‰":
            width, height = self.size_presets[size_preset]
        
        negative_prompt = "lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry,(worst quality:2),(low quality:2),(normal quality:2),lowres,normal quality,((monochrome)),((grayscale)),skin spots,acnes,skin blemishes,age spot,(ugly:1.33),(duplicate:1.33),(morbid:1.21),(mutilated:1.21),(tranny:1.33),mutated hands,(poorly drawn hands:1.5),blurry,(bad anatomy:1.21),(bad proportions:1.33),extra limbs,(disfigured:1.33),(missing arms:1.33),(extra legs:1.33),(fused fingers:1.61),(too many fingers:1.61),(unclear eyes:1.33),lowers,bad hands,missing fingers,extra digit,bad hands,missing fingers,(((extra arms and legs))),DeepNegativeV1.x_V175T,EasyNegative,EasyNegativeV2,"

        payload = {
            'model': model,
            'prompt': prompt,
            'negative_prompt': negative_prompt,
            'steps': steps,
            'guidance': guidance,
            'sampler': sampler,
            'size': f'{width}x{height}',
            'seed': seed
        }

        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }

        try:
            import time
            start_time = time.time()
            
            response = requests.post(
                'https://api-inference.modelscope.cn/v1/images/generations',
                data=json.dumps(payload, ensure_ascii=False).encode('utf-8'),
                headers=headers
            )
            response_data = response.json()
            
            # è®¡ç®—æ¸²æŸ“æ—¶é—´
            render_time = time.time() - start_time
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            if 'images' not in response_data or not response_data['images']:
                error_msg = response_data.get('message', 'æœªçŸ¥é”™è¯¯')
                logger.error(f"API è¿”å›é”™è¯¯: {error_msg}")
                empty_image = torch.zeros((1, 64, 64, 3))
                return (empty_image, f"API è°ƒç”¨å¤±è´¥: {error_msg}", "", 0, 0.0, "", 0, 0, 0, "")

            image_url = response_data['images'][0]['url']
            
            # æ‰“å° URL å’Œæ¸²æŸ“æ—¶é—´
            print(f"\nç”Ÿæˆå›¾ç‰‡ URL: {image_url}")
            print(f"æ¸²æŸ“è€—æ—¶: {render_time:.2f} ç§’\n")
            
            # ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºtensor
            print("æ­£åœ¨ä¸‹è½½å›¾åƒ...")
            image_tensor = self._download_image(image_url)
            
            if image_tensor is None:
                logger.error("å›¾åƒä¸‹è½½å¤±è´¥")
                empty_image = torch.zeros((1, 64, 64, 3))
                return (empty_image, image_url, model, steps, guidance, sampler, width, height, seed, prompt)
            
            print(f"å›¾åƒä¸‹è½½æˆåŠŸï¼Œå°ºå¯¸: {image_tensor.shape}")
            
            return (image_tensor, image_url, model, steps, guidance, sampler, width, height, seed, prompt)
            
        except Exception as e:
            logger.error(f"API è°ƒç”¨é”™è¯¯: {e}")
            empty_image = torch.zeros((1, 64, 64, 3))
            return (empty_image, f"API è°ƒç”¨é”™è¯¯: {str(e)}", "", 0, 0.0, "", 0, 0, 0, "")


class B_quyu:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "original_image": ("IMAGE",),
                "screenshot_image": ("IMAGE",)
            }
        }

    RETURN_TYPES = ("INT", "INT")
    RETURN_NAMES = ("X_direction", "Y_direction")
    FUNCTION = "execute"
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"

    def execute(self, original_image, screenshot_image):

        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è°ƒæ•´é€šé“é¡ºåº
        def tensor_to_cv2(tensor):
            # å»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œè½¬æ¢ä¸ºHWC
            arr = tensor.squeeze(0).cpu().numpy()
            # è½¬æ¢ä¸º0-255 uint8
            arr = (arr * 255).astype(np.uint8)
            # RGBè½¬BGR
            return cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)

        try:
            original = tensor_to_cv2(original_image)
            screenshot = tensor_to_cv2(screenshot_image)

            # æ£€æŸ¥æˆªå›¾å°ºå¯¸æ˜¯å¦è¶…è¿‡åŸå§‹å›¾
            if screenshot.shape[0] > original.shape[0] or screenshot.shape[1] > original.shape[1]:
                return (0, 0)

            # æ¨¡æ¿åŒ¹é…
            result = cv2.matchTemplate(original, screenshot, cv2.TM_CCOEFF_NORMED)
            _, _, _, max_loc = cv2.minMaxLoc(result)
            x, y = max_loc

            return (x, y)
        except Exception as e:
            print(f"Error in B_quyu.execute: {e}")
            return (0, 0)


class B_yuhua:
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "left": ("INT", {"default": 50, "min": 0, "max": 1000, "step": 1}),
                "top": ("INT", {"default": 50, "min": 0, "max": 1000, "step": 1}),
                "right": ("INT", {"default": 50, "min": 0, "max": 1000, "step": 1}),
                "bottom": ("INT", {"default": 50, "min": 0, "max": 1000, "step": 1}),
            }
        }
    
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "yuhua_process"
    DESCRIPTION = "å›¾ç‰‡è¾¹ç¼˜ç¾½åŒ–å¤„ç†ï¼Œé€šè¿‡éçº¿æ€§å˜æ¢å®ç°å¹³æ»‘è‡ªç„¶çš„é€æ˜æ¸å˜æ•ˆæœ"
    
    def create_feather_mask(self, width, height, left, top, right, bottom):
        """åˆ›å»ºç¾½åŒ–é®ç½©"""
        # åˆ›å»ºåŸºç¡€é®ç½©
        mask = np.ones((height, width), dtype=np.float32)
        
        # åˆ›å»ºè·ç¦»åœº
        distance_field = np.zeros((height, width), dtype=np.float32)
        
        # è®¡ç®—æ¯ä¸ªåƒç´ åˆ°è¾¹ç¼˜çš„æœ€å°è·ç¦»
        for y in range(height):
            for x in range(width):
                # è®¡ç®—åˆ°å››è¾¹çš„è·ç¦»
                dist_left = x if left > 0 else float('inf')
                dist_top = y if top > 0 else float('inf')
                dist_right = (width - 1 - x) if right > 0 else float('inf')
                dist_bottom = (height - 1 - y) if bottom > 0 else float('inf')
                
                # å–æœ€å°è·ç¦»
                min_dist = min(dist_left, dist_top, dist_right, dist_bottom)
                
                # å¦‚æœåœ¨ç¾½åŒ–åŒºåŸŸå†…
                if min_dist < max(left, top, right, bottom):
                    distance_field[y, x] = min_dist
        
        # åº”ç”¨éçº¿æ€§å˜æ¢ä¼˜åŒ–è¿‡æ¸¡æ•ˆæœ (ä½¿ç”¨å¹³æ»‘çš„Så‹æ›²çº¿)
        max_feather = max(left, top, right, bottom)
        if max_feather > 0:
            # å½’ä¸€åŒ–è·ç¦»åˆ°0-1èŒƒå›´
            normalized_distance = np.clip(distance_field / max_feather, 0, 1)
            # åº”ç”¨å¹³æ»‘çš„Så‹æ›²çº¿: 3tÂ² - 2tÂ³ (å¹³æ»‘stepå‡½æ•°)
            smooth_factor = 3 * normalized_distance * normalized_distance - 2 * normalized_distance * normalized_distance * normalized_distance
            # è½¬æ¢ä¸ºé€æ˜åº¦ (è¾¹ç¼˜ä¸º0ï¼Œä¸­å¿ƒä¸º1)
            mask = 1 - smooth_factor
        
        return mask
    
    def yuhua_process(self, image, left, top, right, bottom):
        # ç¡®ä¿è¾“å…¥å€¼ä¸ä¸ºè´Ÿæ•°
        left, top, right, bottom = max(0, left), max(0, top), max(0, right), max(0, bottom)
        
        # å¦‚æœæ‰€æœ‰ç¾½åŒ–å€¼éƒ½ä¸º0ï¼Œç›´æ¥è¿”å›åŸå›¾
        if left == 0 and top == 0 and right == 0 and bottom == 0:
            return (image,)
        
        # å°†tensorè½¬æ¢ä¸ºnumpyæ•°ç»„
        batch_size, height, width, channels = image.shape
        output_images = []
        
        for i in range(batch_size):
            # è·å–å•å¼ å›¾ç‰‡
            img_tensor = image[i]
            
            # è½¬æ¢ä¸ºnumpyå¹¶ç¡®ä¿æ˜¯float32
            img_np = img_tensor.cpu().numpy().astype(np.float32)
            
            # å¦‚æœæ˜¯RGBå›¾ç‰‡ï¼Œæ·»åŠ alphaé€šé“
            if channels == 3:
                # åˆ›å»ºRGBAå›¾åƒ
                rgb_img = (img_np * 255).astype(np.uint8)
                pil_img = Image.fromarray(rgb_img, 'RGB')
                rgba_img = pil_img.convert('RGBA')
                img_with_alpha = np.array(rgba_img).astype(np.float32) / 255.0
            else:
                # å·²ç»æ˜¯RGBAæ ¼å¼
                img_with_alpha = img_np.copy()
            
            # åˆ›å»ºç¾½åŒ–é®ç½©
            feather_mask = self.create_feather_mask(width, height, left, top, right, bottom)
            
            # åº”ç”¨ç¾½åŒ–é®ç½©åˆ°alphaé€šé“
            img_with_alpha[:, :, 3] = img_with_alpha[:, :, 3] * feather_mask
            
            # è½¬æ¢å›tensorå¹¶æ·»åŠ åˆ°æ‰¹æ¬¡
            output_img = torch.from_numpy(img_with_alpha).unsqueeze(0)
            output_images.append(output_img)
        
        # åˆå¹¶æ‰¹æ¬¡
        result = torch.cat(output_images, dim=0)
        
        # ç¡®ä¿è¾“å‡ºæ ¼å¼ç¬¦åˆComfyUIè¦æ±‚ (è½¬æ¢ä¸ºuint8ç±»å‹çš„tensor)
        return (result,)


class B_touming:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "left": ("INT", {"default": 150, "min": 0, "max": 1000, "step": 1}),
                "top": ("INT", {"default": 150, "min": 0, "max": 1000, "step": 1}),
                "right": ("INT", {"default": 150, "min": 0, "max": 1000, "step": 1}),
                "bottom": ("INT", {"default": 150, "min": 0, "max": 1000, "step": 1}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "process"
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"

    def process(self, image, left, top, right, bottom):
        # å¤„ç†è¾“å…¥å›¾åƒ
        batch_size, height, width, channels = image.shape
        
        result_batch = []
        
        for i in range(batch_size):
            # è·å–å•å¼ å›¾ç‰‡
            img_tensor = image[i]  # (H, W, C)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_np = img_tensor.cpu().numpy()  # (H, W, C)
            
            # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
            img_np = np.clip(img_np, 0, 1)
            
            # å¦‚æœæ˜¯RGBå›¾åƒï¼Œè½¬æ¢ä¸ºRGBAï¼ˆæ·»åŠ alphaé€šé“ï¼‰
            if channels == 3:
                # åˆ›å»ºalphaé€šé“ï¼Œåˆå§‹åŒ–ä¸ºå…¨ä¸é€æ˜
                alpha_channel = np.ones((height, width, 1), dtype=np.float32)
                img_rgba = np.concatenate([img_np, alpha_channel], axis=2)  # (H, W, 4)
            else:
                img_rgba = img_np.copy()  # å·²ç»æ˜¯RGBAæ ¼å¼
            
            # åˆ›å»ºalphaé®ç½©
            alpha_mask = np.ones((height, width), dtype=np.float32)
            
            # ä¸ºæ¯ä¸ªè¾¹ç¼˜åˆ›å»ºæ¸å˜é€æ˜æ•ˆæœ
            self._create_smooth_gradient(alpha_mask, left, top, right, bottom, height, width)
            
            # åº”ç”¨alphaé®ç½©åˆ°alphaé€šé“
            img_rgba[:, :, 3] = alpha_mask
            
            # è½¬æ¢ä¸ºtensor
            result_tensor = torch.from_numpy(img_rgba).float()
            result_batch.append(result_tensor)
        
        # åˆå¹¶æ‰¹æ¬¡
        result = torch.stack(result_batch, dim=0)
        
        return (result,)

    def _create_smooth_gradient(self, alpha_mask, left, top, right, bottom, height, width):
        """åˆ›å»ºå¹³æ»‘çš„è¾¹ç¼˜é€æ˜æ¸å˜é®ç½©"""
        # å·¦è¾¹ç¼˜æ¸å˜
        if left > 0:
            for x in range(min(left, width)):
                # ä½¿ç”¨å¹³æ»‘çš„è¡°å‡å‡½æ•°
                t = x / left
                # ä½¿ç”¨smoothstepå‡½æ•°åˆ›å»ºå¹³æ»‘è¿‡æ¸¡: 3tÂ² - 2tÂ³
                gradient_value = 3 * t * t - 2 * t * t * t
                current_alpha = alpha_mask[:, x]
                alpha_mask[:, x] = current_alpha * gradient_value
        
        # å³è¾¹ç¼˜æ¸å˜
        if right > 0:
            for x in range(max(0, width - right), width):
                t = (width - 1 - x) / right
                gradient_value = 3 * t * t - 2 * t * t * t
                current_alpha = alpha_mask[:, x]
                alpha_mask[:, x] = current_alpha * gradient_value
        
        # ä¸Šè¾¹ç¼˜æ¸å˜
        if top > 0:
            for y in range(min(top, height)):
                t = y / top
                gradient_value = 3 * t * t - 2 * t * t * t
                current_alpha = alpha_mask[y, :]
                alpha_mask[y, :] = current_alpha * gradient_value
        
        # ä¸‹è¾¹ç¼˜æ¸å˜
        if bottom > 0:
            for y in range(max(0, height - bottom), height):
                t = (height - 1 - y) / bottom
                gradient_value = 3 * t * t - 2 * t * t * t
                current_alpha = alpha_mask[y, :]
                alpha_mask[y, :] = current_alpha * gradient_value

                
class B_hebin:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "background_image": ("IMAGE",),
                "watermark_image": ("IMAGE",),
                "position": (["Centered", "Up", "Down", "Left", "Right", "Up Left", "Up Right", "Down Left", "Down Right"], {"default": "Up Left"}),
                "x_direction": ("INT", {"default": 0, "min": -10000, "max": 10000, "step": 1}),
                "y_direction": ("INT", {"default": 0, "min": -10000, "max": 10000, "step": 1}),
            },
            "optional": {
                "mask": ("MASK",),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "composite"
    CATEGORY = "ğŸ‡¨ğŸ‡³BOZO/PIC"

    def composite(self, background_image, watermark_image, position, x_direction, y_direction, mask=None):
        # è·å–å›¾åƒå°ºå¯¸
        bg_batch, bg_h, bg_w, bg_c = background_image.shape
        wm_batch, wm_h, wm_w, wm_c = watermark_image.shape
        
        # ç¡®å®šæ‰¹æ¬¡å¤§å°
        batch_size = max(bg_batch, wm_batch)
        
        result_batch = []
        
        for i in range(batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡çš„å›¾åƒ
            bg_idx = min(i, bg_batch - 1)
            wm_idx = min(i, wm_batch - 1)
            
            bg_img = background_image[bg_idx].clone()  # (H, W, C)
            wm_img = watermark_image[wm_idx].clone()   # (H, W, C)
            
            # ç¡®ä¿èƒŒæ™¯å›¾æ˜¯RGBæ ¼å¼
            if bg_c == 4:
                bg_rgb = bg_img[:, :, :3]  # æå–RGBé€šé“
            else:
                bg_rgb = bg_img
            
            # ç¡®ä¿æ°´å°å›¾æœ‰alphaé€šé“
            if wm_c == 3:
                # ä¸ºRGBæ°´å°å›¾æ·»åŠ å…¨é€æ˜é€šé“
                alpha_channel = torch.ones((wm_h, wm_w, 1), dtype=wm_img.dtype, device=wm_img.device)
                wm_rgba = torch.cat([wm_img, alpha_channel], dim=2)  # (H, W, 4)
            else:
                wm_rgba = wm_img  # å·²ç»æ˜¯RGBAæ ¼å¼
            
            # å¦‚æœæä¾›äº†maskï¼Œä½¿ç”¨maskä½œä¸ºalphaé€šé“
            if mask is not None:
                mask_batch, mask_h, mask_w = mask.shape
                mask_idx = min(i, mask_batch - 1)
                mask_tensor = mask[mask_idx]  # (H, W)
                
                # è°ƒæ•´maskå°ºå¯¸ä»¥åŒ¹é…æ°´å°å›¾
                if mask_h != wm_h or mask_w != wm_w:
                    mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
                    mask_tensor = torch.nn.functional.interpolate(
                        mask_tensor, size=(wm_h, wm_w), mode='bilinear', align_corners=False
                    ).squeeze(0).squeeze(0)  # (H, W)
                
                # ä½¿ç”¨maskä½œä¸ºalphaé€šé“
                wm_rgba[:, :, 3] = mask_tensor
            
            # è®¡ç®—æ°´å°ä½ç½®
            x_pos, y_pos = self._calculate_position(position, bg_w, bg_h, wm_w, wm_h, x_direction, y_direction)
            
            # ç¡®ä¿ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
            x_pos = max(0, min(x_pos, bg_w))
            y_pos = max(0, min(y_pos, bg_h))
            
            # æ‰§è¡Œå›¾åƒåˆæˆ
            result_img = self._composite_images(bg_rgb, wm_rgba, x_pos, y_pos)
            
            result_batch.append(result_img)
        
        # åˆå¹¶æ‰¹æ¬¡
        result = torch.stack(result_batch, dim=0)
        
        return (result,)

    def _calculate_position(self, position, bg_w, bg_h, wm_w, wm_h, x_direction, y_direction):
        """è®¡ç®—æ°´å°ä½ç½®"""
        positions = {
            "Centered": ((bg_w - wm_w) // 2, (bg_h - wm_h) // 2),
            "Up": ((bg_w - wm_w) // 2, 0),
            "Down": ((bg_w - wm_w) // 2, bg_h - wm_h),
            "Left": (0, (bg_h - wm_h) // 2),
            "Right": (bg_w - wm_w, (bg_h - wm_h) // 2),
            "Up Left": (0, 0),
            "Up Right": (bg_w - wm_w, 0),
            "Down Left": (0, bg_h - wm_h),
            "Down Right": (bg_w - wm_w, bg_h - wm_h),
        }
        
        base_x, base_y = positions.get(position, (0, 0))
        
        # åº”ç”¨æ–¹å‘åç§»
        final_x = base_x + x_direction
        final_y = base_y + y_direction
        
        return final_x, final_y

    def _composite_images(self, bg_img, wm_img, x_pos, y_pos):
        """åˆæˆå›¾åƒ"""
        bg_h, bg_w, bg_c = bg_img.shape
        wm_h, wm_w, wm_c = wm_img.shape
        
        # åˆ›å»ºç»“æœå›¾åƒï¼ˆå¤åˆ¶èƒŒæ™¯ï¼‰
        result_img = bg_img.clone()
        
        # è®¡ç®—å®é™…ç»˜åˆ¶åŒºåŸŸ
        draw_x_start = max(0, x_pos)
        draw_y_start = max(0, y_pos)
        draw_x_end = min(bg_w, x_pos + wm_w)
        draw_y_end = min(bg_h, y_pos + wm_h)
        
        # è®¡ç®—æ°´å°çš„å¯¹åº”åŒºåŸŸ
        wm_x_start = max(0, -x_pos)
        wm_y_start = max(0, -y_pos)
        wm_x_end = wm_x_start + (draw_x_end - draw_x_start)
        wm_y_end = wm_y_start + (draw_y_end - draw_y_start)
        
        if draw_x_end > draw_x_start and draw_y_end > draw_y_start:
            # æå–æ°´å°åŒºåŸŸ
            wm_region = wm_img[wm_y_start:wm_y_end, wm_x_start:wm_x_end, :]  # (H, W, 4)
            bg_region = result_img[draw_y_start:draw_y_end, draw_x_start:draw_x_end, :]  # (H, W, 3)
            
            # è·å–alphaé€šé“å¹¶æ‰©å±•ä¸º3é€šé“
            alpha = wm_region[:, :, 3:4]  # (H, W, 1)
            
            # æå–æ°´å°çš„RGBé€šé“
            wm_rgb = wm_region[:, :, :3]  # (H, W, 3)
            
            # æ‰§è¡Œalphaæ··åˆ
            blended = alpha * wm_rgb + (1 - alpha) * bg_region
            
            # å°†åˆæˆç»“æœå†™å›èƒŒæ™¯å›¾
            result_img[draw_y_start:draw_y_end, draw_x_start:draw_x_end, :] = blended
        
        return result_img
